{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"SCD2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import os\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Defining Various pararmeters in order to make code generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_table=\"/data\\scd2\\source.txt\" ## this path could be a hive table as well\n",
    "target_table=\"/data//scd2//target//target.csv\" ## this path could be a hive table as well\n",
    "table_schema=\"id,name,dob,city,state,country,loaddate\" # common columns in source and target table\n",
    "joining_keys=\"id\" #column used to perform join between source and target \n",
    "scd_columns=\"name,dob,city,state,country\" # columns which will be used to perform SCD\n",
    "multi_day=\"loaddate\" # column which will be used to perform multiday SCD\n",
    "hashing_column=\"surrogate_key\" # hashed value stored in target in order to store hash value\n",
    "scd_helpers=\"start_date,end_date,is_active\" #columns which will be used to identify SCDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+----+-----------+-------+----------+\n",
      "| id|         name|       dob|city|      state|country|  loaddate|\n",
      "+---+-------------+----------+----+-----------+-------+----------+\n",
      "|  1|       AKSHAY|1989-11-13|PUNE|MAHARASHTRA|  INDIA|2019-03-10|\n",
      "|  1|  AKSHAY JAIN|1989-11-13|PUNE|MAHARASHTRA|  INDIA|2019-03-11|\n",
      "|  2|  MONIKA JAIN|1992-07-26|PUNE|MAHARASHTRA|  INDIA|2019-03-10|\n",
      "|  2|MONIKA A JAIN|1992-07-26|PUNE|MAHARASHTRA|  INDIA|2019-03-11|\n",
      "|  3|   AYUSH JAIN|1991-09-01|PUNE|MAHARASHTRA|  INDIA|2019-03-11|\n",
      "|  3|   AYUSH JAIN|1991-09-01|PUNE|MAHARASHTRA|  INDIA|2019-03-12|\n",
      "+---+-------------+----------+----+-----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source=spark.read.format(\"csv\").load(source_table,header=True)\n",
    "source.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing some keys / variables used for multi day processsing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preparing some variables to be used later for multi day processing\n",
    "existing_rec_keys=joining_keys+','+hashing_column\n",
    "i=0\n",
    "days_to_process=source.select(multi_day).distinct().orderBy(multi_day).rdd.map(lambda x:str(x[0])).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generating HASH values for the source data in order to facilitate comparison of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "source=source.withColumn(hashing_column,f.sha2(f.concat_ws(\"|\",*scd_columns.split(\",\")),256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Data from target table (if exists) else creating a empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----+---+----+-----+-------+--------+----------+--------+---------+\n",
      "|surrogate_key| id|name|dob|city|state|country|loaddate|start_date|end_date|is_active|\n",
      "+-------------+---+----+---+----+-----+-------+--------+----------+--------+---------+\n",
      "+-------------+---+----+---+----+-----+-------+--------+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists(target_table):\n",
    "    target=spark.read.format(\"csv\").load(target_table,header=True)\n",
    "\n",
    "else:\n",
    "    target_full_schema=hashing_column+\",\"+table_schema+\",\"+scd_helpers\n",
    "    target_schema=t.StructType([t.StructField(fieldname,t.StringType(),True) for fieldname in target_full_schema.split(\",\")])\n",
    "    target=spark.createDataFrame(sc.emptyRDD(),target_schema)\n",
    "    \n",
    "target.show()\n",
    "target_schema=target.columns\n",
    "#target_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019-03-10', '2019-03-11', '2019-03-12']"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "source_full=source\n",
    "days_to_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCD2 Implementation\n",
    "    Below is the code performing multi day SCD on test dataset, which can work with any data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SCD2 started for ', '2019-03-10')\n",
      "('Data loading completed for ', '2019-03-10')\n",
      "('SCD2 started for ', '2019-03-11')\n",
      "('Data loading completed for ', '2019-03-11')\n",
      "('SCD2 started for ', '2019-03-12')\n",
      "('Data loading completed for ', '2019-03-12')\n",
      "Data loading completed for all days\n"
     ]
    }
   ],
   "source": [
    "while i<len(days_to_process):\n",
    "    source=source_full.filter(condition=multi_day+\"=='\"+str(days_to_process[i])+\"'\")\n",
    "    print('SCD2 started for ',days_to_process[i])\n",
    "    new_rec=source.alias('s').join(target.alias('t'),how=\"left\",on=joining_keys.split(\",\")).filter(condition=\"t.\"+joining_keys+\" is null\").select(\"s.*\").withColumn('start_date',f.col('loaddate')).withColumn('end_date',f.lit('9999-12-31')).withColumn('is_active',f.lit('Y')).selectExpr(*target_schema)\n",
    "    update_rec=source.alias('s').join(target.alias('t'),how=\"inner\",on=joining_keys.split(\",\")).filter(condition=\"s.\"+hashing_column+\"!=t.\"+hashing_column).select(\"s.*\").withColumn('start_date',f.col('s.loaddate')).withColumn('end_date',f.lit('9999-12-31')).withColumn('is_active',f.lit('Y')).selectExpr(*target_schema)\n",
    "    update_old_rec=source.alias('s').join(target.alias('t'),how=\"inner\",on=joining_keys.split(\",\")).filter(condition=\"s.\"+hashing_column+\"!=t.\"+hashing_column).select(\"t.*\",\"s.\"+joining_keys+\"\").withColumn('end_date',f.col('t.loaddate')).withColumn('is_active',f.lit('N')).selectExpr(*target_schema)\n",
    "    existing_rec=source.alias('s').join(target.alias('t'),how=\"inner\",on=existing_rec_keys.split(\",\")).select(joining_keys,hashing_column,'t.*').selectExpr(*target_schema)\n",
    "    delete_rec=source.alias('s').join(target.alias('t'),how=\"right\",on=joining_keys.split(\",\")).filter(condition=\"s.\"+joining_keys+\" is null\").withColumn('end_date',f.col('t.'+multi_day)).withColumn('is_active',f.lit('N')).select('t.*','end_date','is_active').selectExpr(*target_schema)\n",
    "    target_final=new_rec.unionAll(update_rec).unionAll(existing_rec).unionAll(delete_rec).unionAll(update_old_rec)\n",
    "    ##Converting spark dataframe to pandas as I am working in windows . Usually overwrite the data directly into HIVE table\n",
    "    target_final.toPandas().to_csv(target_table,index=False)\n",
    "    target=target_final\n",
    "    print('Data loading completed for ',days_to_process[i])\n",
    "    if(i==len(days_to_process)-1):\n",
    "        print('Data loading completed for all days')\n",
    "    i+=1\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
